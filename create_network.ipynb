{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6mynTN0keV4",
    "outputId": "b13e5b22-8fc4-4eba-fdf2-e5d4d4a9ab1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archy1/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2024-11-15 00:25:33.347719: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-15 00:25:33.361813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/archy1/.local/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/archy1/.local/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56grQqYNkwCZ",
    "outputId": "af6a70ba-eb99-446d-dba7-c41481dedf99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business submissions: 3929\n",
      "business comments: 75980\n",
      "climate submissions: 1483\n",
      "climate comments: 14174\n",
      "energy submissions: 2631\n",
      "energy comments: 49671\n",
      "labor submissions: 262\n",
      "labor comments: 1874\n",
      "education submissions: 1584\n",
      "education comments: 21415\n",
      "news submissions: 55639\n",
      "news comments: 7286594\n",
      "Overall Submissions: 65528\n",
      "Overall Comments: 7449708\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cleans text data by removing URLs and HTML entities.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text_series = text_series.str.replace(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|'\n",
    "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
    "\n",
    "    # Remove HTML\n",
    "    text_series = text_series.str.replace('&gt;', '')\n",
    "\n",
    "    return text_series\n",
    "\n",
    "def add_datetime_columns(df: pd.DataFrame, time_column: str = 'created_utc') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'Y' (year) and 'YM' (year-month) columns to the DataFrame based on a timestamp column.\n",
    "    \"\"\"\n",
    "    # Convert timestamp to datetime and extract the year\n",
    "    df['Y'] = pd.to_datetime(df[time_column], unit='s').dt.year\n",
    "\n",
    "    # Extract year-month in 'YYYY-MM' format\n",
    "    df['YM'] = pd.to_datetime(df[time_column], unit='s').dt.strftime('%Y-%m')\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_subreddit(data_root: str, subreddit_names: List[str], min_num_comments: int = 3, min_score: int = -1, years: List[int] = [2016], chunk_size: int = 10**6) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes submissions and comments for a given subreddit.\n",
    "    \"\"\"\n",
    "    all_submissions = pd.DataFrame()\n",
    "    all_comments = pd.DataFrame()\n",
    "\n",
    "    for subreddit_name in subreddit_names:\n",
    "        # Read submissions CSV file\n",
    "        submissions = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_submissions.csv\")\n",
    "\n",
    "        # Add 'Y' and 'YM' columns to submissions DataFrame\n",
    "        submissions = add_datetime_columns(submissions, 'created_utc')\n",
    "\n",
    "        # Filter submissions by specified years\n",
    "        submissions = submissions[submissions['Y'].isin(years)]\n",
    "\n",
    "        # Filter submissions based on minimum score and number of comments\n",
    "        submissions = submissions[submissions['score'] > min_score]\n",
    "        submissions = submissions[submissions['num_comments'] >= min_num_comments]\n",
    "\n",
    "        # Add subreddit name to DataFrame\n",
    "        submissions['sub'] = subreddit_name\n",
    "\n",
    "        print(f\"{subreddit_name} submissions: {len(submissions)}\")\n",
    "\n",
    "        # Read comments CSV file in chunks\n",
    "        comments_chunks = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_comments.csv\", chunksize=chunk_size)\n",
    "        comments = pd.DataFrame()\n",
    "\n",
    "        for i, chunk in enumerate(comments_chunks):\n",
    "            chunk['link_id'] = chunk['link_id'].str.replace('t3_', '')\n",
    "\n",
    "            # Keep only comments linked to the filtered submissions\n",
    "            chunk = chunk[chunk['link_id'].isin(submissions['id'].unique())]\n",
    "\n",
    "            # Remove prefix from 'parent_id'\n",
    "            chunk['parent_id'] = chunk['parent_id'].str[3:]\n",
    "\n",
    "            # Add 'Y' and 'YM' columns to comments DataFrame\n",
    "            chunk = add_datetime_columns(chunk, 'created_utc')\n",
    "\n",
    "            # Break the loop if the chunk's years are beyond the specified range\n",
    "            if chunk['Y'].min() > max(years):\n",
    "                break\n",
    "\n",
    "            # Clean the 'body' text in comments\n",
    "            chunk['body'] = clean_text(chunk['body'])\n",
    "\n",
    "            # Add subreddit name to DataFrame\n",
    "            chunk['sub'] = subreddit_name\n",
    "\n",
    "            # Concatenate the processed chunk to the main comments DataFrame\n",
    "            comments = pd.concat([comments, chunk], ignore_index=True)\n",
    "\n",
    "        # Remove 't3_' prefix from 'link_id' in comments (redundant but kept for consistency)\n",
    "        comments['link_id'] = comments['link_id'].str.replace('t3_', '')\n",
    "\n",
    "        # Keep only comments linked to the filtered submissions\n",
    "        comments = comments[comments['link_id'].isin(submissions['id'].unique())]\n",
    "\n",
    "        print(f\"{subreddit_name} comments: {len(comments)}\")\n",
    "        \n",
    "        all_submissions = pd.concat([all_submissions, submissions], ignore_index=True)\n",
    "        all_comments = pd.concat([all_comments, comments], ignore_index=True)\n",
    "\n",
    "    return all_submissions, all_comments\n",
    "\n",
    "submissions, comments = process_subreddit(\n",
    "    data_root=\"data/subreddits\", \n",
    "    subreddit_names=[\"business\", \"climate\", \"energy\", \"labor\", \"education\", \"news\"],\n",
    "    min_num_comments=3,\n",
    "    min_score=-1,\n",
    "    years=range(2016, 2017)\n",
    ")\n",
    "\n",
    "# Filter submissions to only include those with comments\n",
    "submissions = submissions[submissions['sub'].isin(comments['sub'].unique())]\n",
    "\n",
    "print('Overall Submissions:', len(submissions))\n",
    "print('Overall Comments:', len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the credibility information for each domain\n",
    "domain_credibility = pd.read_csv(\"data/domain_credibility.csv\", index_col=0, header=0, names=['domain', 'bias', 'credibility'])\n",
    "\n",
    "# Merge credibility information with submissions on domain\n",
    "submissions = submissions.merge(domain_credibility, left_on='domain', right_on='domain', how='left')\n",
    "\n",
    "# Drop submissions with missing credibility information\n",
    "submissions = submissions.dropna(subset=['bias', 'credibility'])\n",
    "\n",
    "# Remove submissions from [deleted] authors\n",
    "submissions = submissions[submissions['author'] != '[deleted]']\n",
    "\n",
    "# Calculate the average credibility rating for each author\n",
    "author_credibility = submissions.groupby('author', as_index=False)['credibility'].mean()\n",
    "\n",
    "# Remove comments from authors with no credibility information\n",
    "comments = comments[comments['author'].isin(author_credibility[\"author\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444370/444370 [14:25<00:00, 513.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Embed the text data for comments\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=\"cuda\")  # You can choose other models as well\n",
    "\n",
    "comments[\"embedding\"] = comments[\"body\"].progress_apply(lambda x: np.array(model.encode(x)))\n",
    "\n",
    "# Calculate the average embedding for each author\n",
    "author_embeddings = comments.groupby('author', as_index=False)['embedding'].apply(lambda x: np.mean(np.vstack(x), axis=0).tolist())\n",
    "\n",
    "embed_dim = len(author_embeddings['embedding'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include links to the original submissions in the comments DataFrame\n",
    "# posts = pd.concat([submissions[[\"author\", \"id\"]].rename(columns={\"id\": \"link_id\"}), comments[[\"author\", \"link_id\"]]])\n",
    "\n",
    "# Calculate the number of shared comments between each pair of authors\n",
    "frequencies_df = pd.crosstab(comments[\"author\"], comments['link_id'])\n",
    "\n",
    "# Associate credibility and embeddings with authors\n",
    "credibilities = frequencies_df.merge(author_credibility, on=\"author\", how='left')[\"credibility\"].to_list()\n",
    "embeddings = frequencies_df.merge(author_embeddings, on=\"author\", how='left')[\"embedding\"].to_list()\n",
    "\n",
    "frequencies = np.array(frequencies_df, dtype=float)\n",
    "\n",
    "adjacency_matrix = frequencies @ frequencies.T\n",
    "np.fill_diagonal(adjacency_matrix, 0)\n",
    "\n",
    "# Connect authors with at least n shared comments\n",
    "adjacency_matrix = (adjacency_matrix >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the adjacency matrix\n",
    "graph = nx.from_numpy_array(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add betweeness centrality, clustering coefficient, degree, and credibility as node attributes\n",
    "betweenness = {i: float(b) for i, b in nx.betweenness_centrality(graph).items()}\n",
    "nx.set_node_attributes(graph, betweenness, 'betweenness')\n",
    "\n",
    "clustering = {i: float(c) for i, c in nx.clustering(graph).items()}\n",
    "nx.set_node_attributes(graph, clustering, 'clustering')\n",
    "\n",
    "degree = dict(nx.degree(graph))\n",
    "nx.set_node_attributes(graph, degree, 'degree')\n",
    "\n",
    "credibility_dict = {i: credibility for i, credibility in enumerate(credibilities)}\n",
    "nx.set_node_attributes(graph, credibility_dict, 'credibility')\n",
    "\n",
    "embedding_dict = {i: {} for i in range(embed_dim)}\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    for j in range(embed_dim):\n",
    "        embedding_dict[j][i] = embedding[j]\n",
    "    \n",
    "for i in range(embed_dim):\n",
    "    nx.set_node_attributes(graph, embedding_dict[i], f'embedding_{i}')\n",
    "\n",
    "nx.write_gexf(graph, 'data/reddit.gexf')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "find",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
