{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6mynTN0keV4",
    "outputId": "b13e5b22-8fc4-4eba-fdf2-e5d4d4a9ab1b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56grQqYNkwCZ",
    "outputId": "af6a70ba-eb99-446d-dba7-c41481dedf99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business: # submissions 265670\n",
      "business: # submissions after cleaning 34342\n",
      "Submissions processing done.\n",
      "Starting processing comments\n",
      "Chunk 0: 7889 comments after filtering\n",
      "Chunk 1: 102534 comments after filtering\n",
      "business: # comments 110423\n",
      "business: # comments after cleaning 110423\n",
      "Finished processing comments\n",
      "Overall Submissions: 34342\n",
      "Overall Comments: 110423\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cleans text data by removing URLs and HTML entities.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text_series = text_series.str.replace(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|'\n",
    "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
    "\n",
    "    # Remove HTML\n",
    "    text_series = text_series.str.replace('&gt;', '')\n",
    "\n",
    "    return text_series\n",
    "\n",
    "def add_datetime_columns(df: pd.DataFrame, time_column: str = 'created_utc') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'Y' (year) and 'YM' (year-month) columns to the DataFrame based on a timestamp column.\n",
    "    \"\"\"\n",
    "    # Convert timestamp to datetime and extract the year\n",
    "    df['Y'] = pd.to_datetime(df[time_column], unit='s').dt.year\n",
    "\n",
    "    # Extract year-month in 'YYYY-MM' format\n",
    "    df['YM'] = pd.to_datetime(df[time_column], unit='s').dt.strftime('%Y-%m')\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_subreddit(data_root: str, subreddit_name: str, min_num_comments: int = 3, min_score: int = -1, years: List[int] = [2016], chunk_size: int = 10**6) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes submissions and comments for a given subreddit.\n",
    "    \"\"\"\n",
    "    # Read submissions CSV file\n",
    "    submissions = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_submissions.csv\")\n",
    "\n",
    "    # Add 'Y' and 'YM' columns to submissions DataFrame\n",
    "    submissions = add_datetime_columns(submissions, 'created_utc')\n",
    "\n",
    "    # Filter submissions by specified years\n",
    "    submissions = submissions[submissions['Y'].isin(years)]\n",
    "    print(f\"{subreddit_name}: # submissions {len(submissions)}\")\n",
    "\n",
    "    # Filter submissions based on minimum score and number of comments\n",
    "    submissions = submissions[submissions['score'] > min_score]\n",
    "    submissions = submissions[submissions['num_comments'] >= min_num_comments]\n",
    "\n",
    "    # Add subreddit name to DataFrame\n",
    "    submissions['sub'] = subreddit_name\n",
    "\n",
    "    print(f\"{subreddit_name}: # submissions after cleaning {len(submissions)}\")\n",
    "    print('Submissions processing done.')\n",
    "\n",
    "    print('Starting processing comments')\n",
    "\n",
    "    # Read comments CSV file in chunks\n",
    "    comments_chunks = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_comments.csv\", chunksize=chunk_size)\n",
    "    comments = pd.DataFrame()\n",
    "\n",
    "    for i, chunk in enumerate(comments_chunks):\n",
    "        chunk['link_id'] = chunk['link_id'].str.replace('t3_', '')\n",
    "\n",
    "        # Keep only comments linked to the filtered submissions\n",
    "        chunk = chunk[chunk['link_id'].isin(submissions['id'].unique())]\n",
    "        print(f\"Chunk {i}: {len(chunk)} comments after filtering\")\n",
    "\n",
    "        # Remove prefix from 'parent_id'\n",
    "        chunk['parent_id'] = chunk['parent_id'].str[3:]\n",
    "\n",
    "        # Add 'Y' and 'YM' columns to comments DataFrame\n",
    "        chunk = add_datetime_columns(chunk, 'created_utc')\n",
    "\n",
    "        # Break the loop if the chunk's years are beyond the specified range\n",
    "        if chunk['Y'].min() > max(years):\n",
    "            break\n",
    "\n",
    "        # Clean the 'body' text in comments\n",
    "        chunk['body'] = clean_text(chunk['body'])\n",
    "\n",
    "        # Add subreddit name to DataFrame\n",
    "        chunk['sub'] = subreddit_name\n",
    "\n",
    "        # Concatenate the processed chunk to the main comments DataFrame\n",
    "        comments = pd.concat([comments, chunk], ignore_index=True)\n",
    "\n",
    "    print(f\"{subreddit_name}: # comments {len(comments)}\")\n",
    "\n",
    "    # Remove 't3_' prefix from 'link_id' in comments (redundant but kept for consistency)\n",
    "    comments['link_id'] = comments['link_id'].str.replace('t3_', '')\n",
    "\n",
    "    # Keep only comments linked to the filtered submissions\n",
    "    comments = comments[comments['link_id'].isin(submissions['id'].unique())]\n",
    "\n",
    "    print(f\"{subreddit_name}: # comments after cleaning {len(comments)}\")\n",
    "    print('Finished processing comments')\n",
    "\n",
    "    return submissions, comments\n",
    "\n",
    "submissions, comments = process_subreddit(\"data\", \"business\", min_num_comments=1, min_score=-1, years=range(2016, 2017))\n",
    "\n",
    "# Filter submissions to only include those with comments in DC\n",
    "submissions = submissions[submissions['sub'].isin(comments['sub'].unique())]\n",
    "\n",
    "print('Overall Submissions:', len(submissions))\n",
    "print('Overall Comments:', len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the credibility information for each domain\n",
    "domain_credibility = pd.read_csv(\"data/domain_credibility.csv\", index_col=0, header=0, names=['domain', 'bias', 'credibility'])\n",
    "\n",
    "# Merge credibility information with submissions on domain\n",
    "submissions = submissions.merge(domain_credibility, left_on='domain', right_on='domain', how='left')\n",
    "\n",
    "# Drop submissions with missing credibility information\n",
    "submissions = submissions.dropna(subset=['bias', 'credibility'])\n",
    "\n",
    "# Remove submissions from [deleted] authors\n",
    "submissions = submissions[submissions['author'] != '[deleted]']\n",
    "\n",
    "# Calculate the average credibility rating for each author\n",
    "author_credibility = submissions.groupby('author', as_index=False)['credibility'].mean()\n",
    "\n",
    "# Remove comments from authors with no credibility information\n",
    "comments = comments[comments['author'].isin(author_credibility[\"author\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include links to the original submissions in the comments DataFrame\n",
    "# posts = pd.concat([submissions[[\"author\", \"id\"]].rename(columns={\"id\": \"link_id\"}), comments[[\"author\", \"link_id\"]]])\n",
    "\n",
    "# Calculate the number of shared comments between each pair of authors\n",
    "frequencies_df = pd.crosstab(comments[\"author\"], comments['link_id'])\n",
    "\n",
    "credibilities = frequencies_df.merge(author_credibility, on=\"author\", how='left')[\"credibility\"].to_list()\n",
    "\n",
    "frequencies = np.array(frequencies_df, dtype=float)\n",
    "\n",
    "adjacency_matrix = frequencies @ frequencies.T\n",
    "np.fill_diagonal(adjacency_matrix, 0)\n",
    "\n",
    "# Connect authors with at least n shared comments\n",
    "adjacency_matrix = (adjacency_matrix >= 1).astype(int)\n",
    "\n",
    "# Create a graph from the adjacency matrix\n",
    "graph = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "# Add betweeness centrality, clustering coefficient, degree, and credibility as node attributes\n",
    "betweenness = {i: float(b) for i, b in nx.betweenness_centrality(graph).items()}\n",
    "nx.set_node_attributes(graph, betweenness, 'betweenness')\n",
    "\n",
    "clustering = {i: float(c) for i, c in nx.clustering(graph).items()}\n",
    "nx.set_node_attributes(graph, clustering, 'clustering')\n",
    "\n",
    "degree = dict(nx.degree(graph))\n",
    "nx.set_node_attributes(graph, degree, 'degree')\n",
    "\n",
    "# author_names = {i: author for i, author in enumerate(frequencies_df.index)}\n",
    "# nx.set_node_attributes(graph, author_names, 'author')\n",
    "\n",
    "credibility = {i: int(credibility > 0.5) for i, credibility in enumerate(credibilities)}\n",
    "nx.set_node_attributes(graph, credibility, 'credibility')\n",
    "\n",
    "nx.write_gexf(graph, 'data/business_users.gexf')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "find",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
