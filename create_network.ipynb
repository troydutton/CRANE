{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6mynTN0keV4",
        "outputId": "b13e5b22-8fc4-4eba-fdf2-e5d4d4a9ab1b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "from typing import List, Tuple\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56grQqYNkwCZ",
        "outputId": "af6a70ba-eb99-446d-dba7-c41481dedf99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "business: # submissions 265670\n",
            "business: # submissions after cleaning 265670\n",
            "Submissions processing done.\n",
            "Starting processing comments\n",
            "Chunk 0: 7913 comments after filtering\n",
            "Chunk 1: 103515 comments after filtering\n",
            "business: # comments 111428\n",
            "business: # comments after cleaning 111428\n",
            "Finished processing comments\n",
            "Overall Submissions: 265670\n",
            "Overall Comments: 111428\n"
          ]
        }
      ],
      "source": [
        "def clean_text(text_series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Cleans text data by removing URLs and HTML entities.\n",
        "    \"\"\"\n",
        "    # Remove URLs\n",
        "    text_series = text_series.str.replace(\n",
        "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|'\n",
        "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
        "\n",
        "    # Remove HTML\n",
        "    text_series = text_series.str.replace('&gt;', '')\n",
        "\n",
        "    return text_series\n",
        "\n",
        "def add_datetime_columns(df: pd.DataFrame, time_column: str = 'created_utc') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds 'Y' (year) and 'YM' (year-month) columns to the DataFrame based on a timestamp column.\n",
        "    \"\"\"\n",
        "    # Convert timestamp to datetime and extract the year\n",
        "    df['Y'] = pd.to_datetime(df[time_column], unit='s').dt.year\n",
        "\n",
        "    # Extract year-month in 'YYYY-MM' format\n",
        "    df['YM'] = pd.to_datetime(df[time_column], unit='s').dt.strftime('%Y-%m')\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_subreddit(data_root: str, subreddit_name: str, min_num_comments: int = 3, min_score: int = -1, years: List[int] = [2016], chunk_size: int = 10**6) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Processes submissions and comments for a given subreddit.\n",
        "    \"\"\"\n",
        "    # Read submissions CSV file\n",
        "    submissions = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_submissions.csv\")\n",
        "\n",
        "    # Add 'Y' and 'YM' columns to submissions DataFrame\n",
        "    submissions = add_datetime_columns(submissions, 'created_utc')\n",
        "\n",
        "    # Filter submissions by specified years\n",
        "    submissions = submissions[submissions['Y'].isin(years)]\n",
        "    print(f\"{subreddit_name}: # submissions {len(submissions)}\")\n",
        "\n",
        "    # Filter submissions based on minimum score and number of comments\n",
        "    submissions = submissions[submissions['score'] > min_score]\n",
        "    submissions = submissions[submissions['num_comments'] >= min_num_comments]\n",
        "\n",
        "    # Add subreddit name to DataFrame\n",
        "    submissions['sub'] = subreddit_name\n",
        "\n",
        "    print(f\"{subreddit_name}: # submissions after cleaning {len(submissions)}\")\n",
        "    print('Submissions processing done.')\n",
        "\n",
        "    print('Starting processing comments')\n",
        "\n",
        "    # Read comments CSV file in chunks\n",
        "    comments_chunks = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_comments.csv\", chunksize=chunk_size)\n",
        "    comments = pd.DataFrame()\n",
        "\n",
        "    for i, chunk in enumerate(comments_chunks):\n",
        "        chunk['link_id'] = chunk['link_id'].str.replace('t3_', '')\n",
        "\n",
        "        # Keep only comments linked to the filtered submissions\n",
        "        chunk = chunk[chunk['link_id'].isin(submissions['id'].unique())]\n",
        "        print(f\"Chunk {i}: {len(chunk)} comments after filtering\")\n",
        "\n",
        "        # Remove prefix from 'parent_id'\n",
        "        chunk['parent_id'] = chunk['parent_id'].str[3:]\n",
        "\n",
        "        # Add 'Y' and 'YM' columns to comments DataFrame\n",
        "        chunk = add_datetime_columns(chunk, 'created_utc')\n",
        "\n",
        "        # Break the loop if the chunk's years are beyond the specified range\n",
        "        if chunk['Y'].min() > max(years):\n",
        "            break\n",
        "\n",
        "        # Clean the 'body' text in comments\n",
        "        chunk['body'] = clean_text(chunk['body'])\n",
        "\n",
        "        # Add subreddit name to DataFrame\n",
        "        chunk['sub'] = subreddit_name\n",
        "\n",
        "        # Concatenate the processed chunk to the main comments DataFrame\n",
        "        comments = pd.concat([comments, chunk], ignore_index=True)\n",
        "\n",
        "    print(f\"{subreddit_name}: # comments {len(comments)}\")\n",
        "\n",
        "    # Remove 't3_' prefix from 'link_id' in comments (redundant but kept for consistency)\n",
        "    comments['link_id'] = comments['link_id'].str.replace('t3_', '')\n",
        "\n",
        "    # Keep only comments linked to the filtered submissions\n",
        "    comments = comments[comments['link_id'].isin(submissions['id'].unique())]\n",
        "\n",
        "    print(f\"{subreddit_name}: # comments after cleaning {len(comments)}\")\n",
        "    print('Finished processing comments')\n",
        "\n",
        "    return submissions, comments\n",
        "\n",
        "submissions, comments = process_subreddit(\"data\", \"business\", min_num_comments=0, min_score=-1, years=range(2016, 2017))\n",
        "\n",
        "# Filter submissions to only include those with comments in DC\n",
        "submissions = submissions[submissions['sub'].isin(comments['sub'].unique())]\n",
        "\n",
        "print('Overall Submissions:', len(submissions))\n",
        "print('Overall Comments:', len(comments))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>author_flair_text</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>domain</th>\n",
              "      <th>title</th>\n",
              "      <th>Y</th>\n",
              "      <th>YM</th>\n",
              "      <th>sub</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>797080</th>\n",
              "      <td>797080</td>\n",
              "      <td>3yyy1u</td>\n",
              "      <td>ElizabethNarula</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1451608233</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>prdaily.com</td>\n",
              "      <td>Starbucks cookies and more.</td>\n",
              "      <td>2016</td>\n",
              "      <td>2016-01</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797081</th>\n",
              "      <td>797081</td>\n",
              "      <td>3yyzmg</td>\n",
              "      <td>awesomer121</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1451609122</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>itunes.apple.com</td>\n",
              "      <td>A 14 year old launches an application that wil...</td>\n",
              "      <td>2016</td>\n",
              "      <td>2016-01</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797082</th>\n",
              "      <td>797082</td>\n",
              "      <td>3yz1ap</td>\n",
              "      <td>basementguys</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1451610053</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>adpost.com</td>\n",
              "      <td>INSULATION PRODUCTS</td>\n",
              "      <td>2016</td>\n",
              "      <td>2016-01</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797083</th>\n",
              "      <td>797083</td>\n",
              "      <td>3yz45l</td>\n",
              "      <td>ElizabethNarula</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1451611853</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>marketingland.com</td>\n",
              "      <td>Top 10 Video Creators in November: The Ellen S...</td>\n",
              "      <td>2016</td>\n",
              "      <td>2016-01</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797084</th>\n",
              "      <td>797084</td>\n",
              "      <td>3yz8sl</td>\n",
              "      <td>donnagain</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1451614722</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>youtube.com</td>\n",
              "      <td>Day 29 It's Time To Burry Your Ego</td>\n",
              "      <td>2016</td>\n",
              "      <td>2016-01</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Index      id           author author_flair_text  created_utc  \\\n",
              "797080  797080  3yyy1u  ElizabethNarula               NaN   1451608233   \n",
              "797081  797081  3yyzmg      awesomer121               NaN   1451609122   \n",
              "797082  797082  3yz1ap     basementguys               NaN   1451610053   \n",
              "797083  797083  3yz45l  ElizabethNarula               NaN   1451611853   \n",
              "797084  797084  3yz8sl        donnagain               NaN   1451614722   \n",
              "\n",
              "        num_comments  score             domain  \\\n",
              "797080             0      1        prdaily.com   \n",
              "797081             5      0   itunes.apple.com   \n",
              "797082             0      1         adpost.com   \n",
              "797083             0      1  marketingland.com   \n",
              "797084             0      1        youtube.com   \n",
              "\n",
              "                                                    title     Y       YM  \\\n",
              "797080                        Starbucks cookies and more.  2016  2016-01   \n",
              "797081  A 14 year old launches an application that wil...  2016  2016-01   \n",
              "797082                                INSULATION PRODUCTS  2016  2016-01   \n",
              "797083  Top 10 Video Creators in November: The Ellen S...  2016  2016-01   \n",
              "797084                 Day 29 It's Time To Burry Your Ego  2016  2016-01   \n",
              "\n",
              "             sub  \n",
              "797080  business  \n",
              "797081  business  \n",
              "797082  business  \n",
              "797083  business  \n",
              "797084  business  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submissions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the number of shared comments between each pair of authors\n",
        "frequencies = np.array(pd.crosstab(comments['author'], comments['link_id'])).astype(float)\n",
        "\n",
        "adjacency_matrix = frequencies @ frequencies.T\n",
        "np.fill_diagonal(adjacency_matrix, 0)\n",
        "\n",
        "# Connect authors with at least n shared comments\n",
        "adjacency_matrix = adjacency_matrix >= 2\n",
        "\n",
        "# Take a subset of the adjacency matrix\n",
        "adjacency_matrix = adjacency_matrix[:1000, :1000]\n",
        "\n",
        "# Write the adjacency matrix to a GEXF file\n",
        "graph = nx.from_numpy_array(adjacency_matrix)\n",
        "nx.write_gexf(graph, 'data/business_users.gexf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the degree of the graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "degrees = np.array([degree for node, degree in graph.degree()])\n",
        "\n",
        "plt.hist(degrees, bins=25, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Degree Distribution')\n",
        "plt.savefig('data/degree_distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comments.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sbm = \"3z2f4k\"\n",
        "\n",
        "# Assuming DC and sbm are defined somewhere earlier in your code\n",
        "data = comments[comments['link_id'] == sbm]\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "# Filter necessary columns\n",
        "filtered_comments = data[['id', 'parent_id', 'body']]\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Define a root node for visualization and add nodes and edges\n",
        "root_node = 'Root'\n",
        "G.add_node(root_node, body=\"Root Post\")\n",
        "for _, row in filtered_comments.iterrows():\n",
        "    G.add_node(row['id'], body=row['id'])  # Use full ID for display\n",
        "    parent_id = row['parent_id'] if not row['parent_id'].startswith(sbm) else root_node\n",
        "    G.add_edge(parent_id, row['id'])\n",
        "\n",
        "# Compute depth for each node for shell assignment\n",
        "depth = nx.single_source_shortest_path_length(G, root_node)\n",
        "# Create shell layout based on depths\n",
        "max_depth = max(depth.values())\n",
        "shells = [[] for _ in range(max_depth + 1)]\n",
        "for node, d in depth.items():\n",
        "    shells[d].append(node)\n",
        "\n",
        "# Define node colors based on depth with a more vibrant color map\n",
        "node_color = [depth[node] / max_depth for node in G.nodes()]\n",
        "\n",
        "if 1:\n",
        "  # Draw the graph using the shell layout\n",
        "  pos = nx.shell_layout(G, shells)\n",
        "else:\n",
        "  pos = nx.spring_layout(G, k=0.1, iterations=50)  # Adjust k and iterations for better layout\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=100, cmap=plt.cm.viridis, alpha=0.8)\n",
        "nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
        "nx.draw_networkx_labels(G, pos, labels={n: G.nodes[n]['body'] for n in G.nodes}, font_size=8)\n",
        "\n",
        "plt.title(\"Comment Tree Visualization\")\n",
        "plt.axis('off')  # Hide axes\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
