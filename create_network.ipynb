{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6mynTN0keV4",
    "outputId": "b13e5b22-8fc4-4eba-fdf2-e5d4d4a9ab1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archy1/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56grQqYNkwCZ",
    "outputId": "af6a70ba-eb99-446d-dba7-c41481dedf99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business submissions: 3929\n",
      "business comments: 75980\n",
      "climate submissions: 1483\n",
      "climate comments: 14174\n",
      "energy submissions: 2631\n",
      "energy comments: 49671\n",
      "labor submissions: 262\n",
      "labor comments: 1874\n",
      "education submissions: 1584\n",
      "education comments: 21415\n",
      "news submissions: 55639\n",
      "news comments: 7286594\n",
      "Overall Submissions: 65528\n",
      "Overall Comments: 7449708\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cleans text data by removing URLs and HTML entities.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text_series = text_series.str.replace(\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|'\n",
    "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
    "\n",
    "    # Remove HTML\n",
    "    text_series = text_series.str.replace('&gt;', '')\n",
    "\n",
    "    return text_series\n",
    "\n",
    "def add_datetime_columns(df: pd.DataFrame, time_column: str = 'created_utc') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds 'Y' (year) and 'YM' (year-month) columns to the DataFrame based on a timestamp column.\n",
    "    \"\"\"\n",
    "    # Convert timestamp to datetime and extract the year\n",
    "    df['Y'] = pd.to_datetime(df[time_column], unit='s').dt.year\n",
    "\n",
    "    # Extract year-month in 'YYYY-MM' format\n",
    "    df['YM'] = pd.to_datetime(df[time_column], unit='s').dt.strftime('%Y-%m')\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_subreddit(data_root: str, subreddit_names: List[str], min_num_comments: int = 3, min_score: int = -1, years: List[int] = [2016], chunk_size: int = 10**6) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Processes submissions and comments for a given subreddit.\n",
    "    \"\"\"\n",
    "    all_submissions = pd.DataFrame()\n",
    "    all_comments = pd.DataFrame()\n",
    "\n",
    "    for subreddit_name in subreddit_names:\n",
    "        # Read submissions CSV file\n",
    "        submissions = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_submissions.csv\")\n",
    "\n",
    "        # Add 'Y' and 'YM' columns to submissions DataFrame\n",
    "        submissions = add_datetime_columns(submissions, 'created_utc')\n",
    "\n",
    "        # Filter submissions by specified years\n",
    "        submissions = submissions[submissions['Y'].isin(years)]\n",
    "\n",
    "        # Filter submissions based on minimum score and number of comments\n",
    "        submissions = submissions[submissions['score'] > min_score]\n",
    "        submissions = submissions[submissions['num_comments'] >= min_num_comments]\n",
    "\n",
    "        # Add subreddit name to DataFrame\n",
    "        submissions['sub'] = subreddit_name\n",
    "\n",
    "        print(f\"{subreddit_name} submissions: {len(submissions)}\")\n",
    "\n",
    "        # Read comments CSV file in chunks\n",
    "        comments_chunks = pd.read_csv(f\"{os.path.join(data_root, subreddit_name)}_comments.csv\", chunksize=chunk_size)\n",
    "        comments = pd.DataFrame()\n",
    "\n",
    "        for i, chunk in enumerate(comments_chunks):\n",
    "            chunk['link_id'] = chunk['link_id'].str.replace('t3_', '')\n",
    "\n",
    "            # Keep only comments linked to the filtered submissions\n",
    "            chunk = chunk[chunk['link_id'].isin(submissions['id'].unique())]\n",
    "\n",
    "            # Remove prefix from 'parent_id'\n",
    "            chunk['parent_id'] = chunk['parent_id'].str[3:]\n",
    "\n",
    "            # Add 'Y' and 'YM' columns to comments DataFrame\n",
    "            chunk = add_datetime_columns(chunk, 'created_utc')\n",
    "\n",
    "            # Break the loop if the chunk's years are beyond the specified range\n",
    "            if chunk['Y'].min() > max(years):\n",
    "                break\n",
    "\n",
    "            # Clean the 'body' text in comments\n",
    "            chunk['body'] = clean_text(chunk['body'])\n",
    "\n",
    "            # Add subreddit name to DataFrame\n",
    "            chunk['sub'] = subreddit_name\n",
    "\n",
    "            # Concatenate the processed chunk to the main comments DataFrame\n",
    "            comments = pd.concat([comments, chunk], ignore_index=True)\n",
    "\n",
    "        # Remove 't3_' prefix from 'link_id' in comments (redundant but kept for consistency)\n",
    "        comments['link_id'] = comments['link_id'].str.replace('t3_', '')\n",
    "\n",
    "        # Keep only comments linked to the filtered submissions\n",
    "        comments = comments[comments['link_id'].isin(submissions['id'].unique())]\n",
    "\n",
    "        print(f\"{subreddit_name} comments: {len(comments)}\")\n",
    "        \n",
    "        all_submissions = pd.concat([all_submissions, submissions], ignore_index=True)\n",
    "        all_comments = pd.concat([all_comments, comments], ignore_index=True)\n",
    "\n",
    "    return all_submissions, all_comments\n",
    "\n",
    "submissions, comments = process_subreddit(\n",
    "    data_root=\"data/subreddits\", \n",
    "    subreddit_names=[\"business\", \"climate\", \"energy\", \"labor\", \"education\", \"news\"],\n",
    "    min_num_comments=3,\n",
    "    min_score=-1,\n",
    "    years=range(2016, 2017)\n",
    ")\n",
    "\n",
    "# Filter submissions to only include those with comments\n",
    "submissions = submissions[submissions['sub'].isin(comments['sub'].unique())]\n",
    "\n",
    "print('Overall Submissions:', len(submissions))\n",
    "print('Overall Comments:', len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['bias', 'credibility']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1491482/1319452977.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Merge credibility information with submissions on domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubmissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_credibility\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Drop submissions with missing credibility information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msubmissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'credibility'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Remove submissions from [deleted] authors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msubmissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubmissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'[deleted]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6666\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ['bias', 'credibility']"
     ]
    }
   ],
   "source": [
    "# Get the credibility information for each domain\n",
    "domain_credibility = pd.read_csv(\"data/domain_credibility.csv\", index_col=0, header=0, names=['domain', 'bias', 'credibility'])\n",
    "\n",
    "# Merge credibility information with submissions on domain\n",
    "submissions = submissions.merge(domain_credibility, left_on='domain', right_on='domain', how='left')\n",
    "\n",
    "# Drop submissions with missing credibility information\n",
    "submissions = submissions.dropna(subset=['bias', 'credibility'])\n",
    "\n",
    "# Remove submissions from [deleted] authors\n",
    "submissions = submissions[submissions['author'] != '[deleted]']\n",
    "\n",
    "# Calculate the average credibility rating for each author\n",
    "author_credibility = submissions.groupby('author', as_index=False)['credibility'].mean()\n",
    "\n",
    "# Remove comments from authors with no credibility information\n",
    "comments = comments[comments['author'].isin(author_credibility[\"author\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444370/444370 [14:55<00:00, 496.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2', device=\"cuda\")\n",
    "comments[\"embedding\"] = comments[\"body\"].progress_apply(lambda x: np.array(sentence_transformer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include links to the original submissions in the comments DataFrame\n",
    "# posts = pd.concat([submissions[[\"author\", \"id\"]].rename(columns={\"id\": \"link_id\"}), comments[[\"author\", \"link_id\"]]])\n",
    "\n",
    "# Calculate the number of shared comments between each pair of authors\n",
    "frequencies_df = pd.crosstab(comments[\"author\"], comments['link_id'])\n",
    "\n",
    "credibilities = frequencies_df.merge(author_credibility, on=\"author\", how='left')[\"credibility\"].to_list()\n",
    "\n",
    "frequencies = np.array(frequencies_df, dtype=float)\n",
    "\n",
    "adjacency_matrix = frequencies @ frequencies.T\n",
    "np.fill_diagonal(adjacency_matrix, 0)\n",
    "\n",
    "# Connect authors with at least n shared comments\n",
    "adjacency_matrix = (adjacency_matrix >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the adjacency matrix\n",
    "graph = nx.from_numpy_array(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>body</th>\n",
       "      <th>Y</th>\n",
       "      <th>YM</th>\n",
       "      <th>sub</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cyic7p5</th>\n",
       "      <td>991812</td>\n",
       "      <td>1451664317</td>\n",
       "      <td>3z0hsy</td>\n",
       "      <td>3z0hsy</td>\n",
       "      <td>We welcome our new lords and masters.</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>business</td>\n",
       "      <td>[0.032246627, 0.012166687, 0.06292515, -0.0576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cz3j7xt</th>\n",
       "      <td>997824</td>\n",
       "      <td>1453174086</td>\n",
       "      <td>41kafd</td>\n",
       "      <td>41kafd</td>\n",
       "      <td>Good.</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01</td>\n",
       "      <td>business</td>\n",
       "      <td>[-0.088112794, -0.012578506, -0.06211385, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>czq50ml</th>\n",
       "      <td>1003413</td>\n",
       "      <td>1454780155</td>\n",
       "      <td>44gecs</td>\n",
       "      <td>44gecs</td>\n",
       "      <td>It was broad-based: The only category where re...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>business</td>\n",
       "      <td>[0.019247988, -0.027884975, 0.09817463, 0.0848...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d01zafr</th>\n",
       "      <td>1006501</td>\n",
       "      <td>1455638118</td>\n",
       "      <td>461wa8</td>\n",
       "      <td>461wa8</td>\n",
       "      <td>Oil price is dropping today heh.</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>business</td>\n",
       "      <td>[-0.018226402, 0.007058047, 0.15740283, 0.0377...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d0su8nd</th>\n",
       "      <td>1013078</td>\n",
       "      <td>1457488583</td>\n",
       "      <td>d0ss9in</td>\n",
       "      <td>49i8z3</td>\n",
       "      <td>Different areas have different drilling/fracki...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>business</td>\n",
       "      <td>[0.07944631, 0.0022230602, 0.0765335, 0.072117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbp5vgt</th>\n",
       "      <td>21630897</td>\n",
       "      <td>1482886411</td>\n",
       "      <td>5kju39</td>\n",
       "      <td>5kju39</td>\n",
       "      <td>Go to bed Israel.  You're drunk.</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>news</td>\n",
       "      <td>[0.14629324, 0.026536558, 0.009616932, -0.0032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbp5ww8</th>\n",
       "      <td>21630903</td>\n",
       "      <td>1482886472</td>\n",
       "      <td>5klivq</td>\n",
       "      <td>5klivq</td>\n",
       "      <td>Wow that UN vote had more effect than I though...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>news</td>\n",
       "      <td>[-0.0006077492, 0.021225117, 0.02102755, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbrr3rj</th>\n",
       "      <td>21668605</td>\n",
       "      <td>1483047197</td>\n",
       "      <td>5kyj7x</td>\n",
       "      <td>5kyj7x</td>\n",
       "      <td>Time to short bitcoin.</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>news</td>\n",
       "      <td>[0.025361173, 0.10327468, 0.005644338, 0.01683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbrzi7j</th>\n",
       "      <td>21672633</td>\n",
       "      <td>1483058733</td>\n",
       "      <td>dbrywow</td>\n",
       "      <td>5kyj7x</td>\n",
       "      <td>Sure.  The old fashioned way.  Borrow some bit...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>news</td>\n",
       "      <td>[-0.03206374, 0.03921009, -0.05505047, -0.0132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbtrrn5</th>\n",
       "      <td>21695035</td>\n",
       "      <td>1483167676</td>\n",
       "      <td>5l7mq3</td>\n",
       "      <td>5l7mq3</td>\n",
       "      <td>Investigators are searching for 32-year-old Ja...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>news</td>\n",
       "      <td>[-0.03128543, 0.012169079, -0.062040977, -0.04...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Index  created_utc parent_id link_id  \\\n",
       "id                                                 \n",
       "cyic7p5    991812   1451664317    3z0hsy  3z0hsy   \n",
       "cz3j7xt    997824   1453174086    41kafd  41kafd   \n",
       "czq50ml   1003413   1454780155    44gecs  44gecs   \n",
       "d01zafr   1006501   1455638118    461wa8  461wa8   \n",
       "d0su8nd   1013078   1457488583   d0ss9in  49i8z3   \n",
       "...           ...          ...       ...     ...   \n",
       "dbp5vgt  21630897   1482886411    5kju39  5kju39   \n",
       "dbp5ww8  21630903   1482886472    5klivq  5klivq   \n",
       "dbrr3rj  21668605   1483047197    5kyj7x  5kyj7x   \n",
       "dbrzi7j  21672633   1483058733   dbrywow  5kyj7x   \n",
       "dbtrrn5  21695035   1483167676    5l7mq3  5l7mq3   \n",
       "\n",
       "                                                      body     Y       YM  \\\n",
       "id                                                                          \n",
       "cyic7p5              We welcome our new lords and masters.  2016  2016-01   \n",
       "cz3j7xt                                            Good.    2016  2016-01   \n",
       "czq50ml  It was broad-based: The only category where re...  2016  2016-02   \n",
       "d01zafr                   Oil price is dropping today heh.  2016  2016-02   \n",
       "d0su8nd  Different areas have different drilling/fracki...  2016  2016-03   \n",
       "...                                                    ...   ...      ...   \n",
       "dbp5vgt                   Go to bed Israel.  You're drunk.  2016  2016-12   \n",
       "dbp5ww8  Wow that UN vote had more effect than I though...  2016  2016-12   \n",
       "dbrr3rj                             Time to short bitcoin.  2016  2016-12   \n",
       "dbrzi7j  Sure.  The old fashioned way.  Borrow some bit...  2016  2016-12   \n",
       "dbtrrn5  Investigators are searching for 32-year-old Ja...  2016  2016-12   \n",
       "\n",
       "              sub                                          embedding  \n",
       "id                                                                    \n",
       "cyic7p5  business  [0.032246627, 0.012166687, 0.06292515, -0.0576...  \n",
       "cz3j7xt  business  [-0.088112794, -0.012578506, -0.06211385, 0.04...  \n",
       "czq50ml  business  [0.019247988, -0.027884975, 0.09817463, 0.0848...  \n",
       "d01zafr  business  [-0.018226402, 0.007058047, 0.15740283, 0.0377...  \n",
       "d0su8nd  business  [0.07944631, 0.0022230602, 0.0765335, 0.072117...  \n",
       "...           ...                                                ...  \n",
       "dbp5vgt      news  [0.14629324, 0.026536558, 0.009616932, -0.0032...  \n",
       "dbp5ww8      news  [-0.0006077492, 0.021225117, 0.02102755, -0.01...  \n",
       "dbrr3rj      news  [0.025361173, 0.10327468, 0.005644338, 0.01683...  \n",
       "dbrzi7j      news  [-0.03206374, 0.03921009, -0.05505047, -0.0132...  \n",
       "dbtrrn5      news  [-0.03128543, 0.012169079, -0.062040977, -0.04...  \n",
       "\n",
       "[109 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_indexed = comments.set_index(['id', 'author'])\n",
    "comments_indexed.xs('eigenman', level='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cred_sim(parent_author_credibility, user_body, parent_id):\n",
    "#     # Get embeddings\n",
    "#     user_embedding = sentence_transformer.encode(user_body).reshape(1, -1)\n",
    "#     parent_embedding = comments_indexed.xs(parent_id, level='author')[\"embedding\"].values[0].reshape(1, -1)\n",
    "\n",
    "#     # Calculate similarity\n",
    "#     similarity = cosine_similarity(user_embedding, parent_embedding)[0][0]\n",
    "    \n",
    "#     # Normalize credibility to [-1, 1]\n",
    "#     parent_author_credibility = (parent_author_credibility * 2 - 1)\n",
    "\n",
    "#     return similarity * parent_author_credibility\n",
    "\n",
    "def get_cred_sim(parent_author_credibility, user_body, parent_body):\n",
    "    # Get embeddings\n",
    "    user_embedding = sentence_transformer.encode(user_body).reshape(1, -1)\n",
    "    parent_embedding = sentence_transformer.encode(parent_body).reshape(1, -1)\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarity = cosine_similarity(user_embedding, parent_embedding)[0][0]\n",
    "    \n",
    "    # Normalize credibility to [-1, 1]\n",
    "    parent_author_credibility = (parent_author_credibility * 2 - 1)\n",
    "\n",
    "    return similarity * parent_author_credibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cred_sims():\n",
    "    # Create df for author similarities\n",
    "    author_cred_sims = pd.DataFrame(columns=['author', 'similarity'])\n",
    "\n",
    "    # Loop through all users\n",
    "    users = comments['author'].unique()\n",
    "    for user in tqdm(users):\n",
    "\n",
    "        # Array for storing similarity values\n",
    "        cred_sims = []\n",
    "\n",
    "        # Loop through all comments\n",
    "        user_comments = comments[comments['author'] == user]\n",
    "        for user_comment in user_comments.iterrows():\n",
    "            user_comment = user_comment[1]\n",
    "            user_body = user_comment['body']\n",
    "\n",
    "            # Get parent id of the comment\n",
    "            parent_id = user_comment['parent_id']\n",
    "\n",
    "            # Get parent comment\n",
    "            parent_comment = comments[comments['id'] == parent_id]\n",
    "\n",
    "            # If the parent comment is not found, skip\n",
    "            if parent_comment.empty:\n",
    "                continue\n",
    "\n",
    "            # Get parent comment body\n",
    "            parent_body = parent_comment['body'].values[0]\n",
    "\n",
    "            # Get author of the parent comment\n",
    "            parent_author = parent_comment['author'].values[0]\n",
    "\n",
    "            # Get author credibility'\n",
    "            parent_author_credibility = author_credibility[author_credibility['author'] == parent_author]['credibility'].values[0]\n",
    "\n",
    "            # Calculate similarity value and store in array\n",
    "            cred_sim = get_cred_sim(parent_author_credibility, user_body, parent_body)\n",
    "            cred_sims.append(cred_sim)\n",
    "\n",
    "        # Check for empty array\n",
    "        if len(cred_sims) == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate average similarity value\n",
    "        avg_cred_sim = sum(cred_sims) / len(cred_sims)\n",
    "\n",
    "        # Add to df\n",
    "        new_row = {'author': parent_author, 'similarity': avg_cred_sim}\n",
    "        author_cred_sims.loc[len(author_cred_sims)] = new_row\n",
    "        \n",
    "    return author_cred_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/6278 [00:19<8:38:07,  4.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cred_sims \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_cred_sims\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 22\u001b[0m, in \u001b[0;36mcreate_cred_sims\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m parent_id \u001b[38;5;241m=\u001b[39m user_comment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get parent comment\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m parent_comment \u001b[38;5;241m=\u001b[39m comments[\u001b[43mcomments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparent_id\u001b[49m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# If the parent comment is not found, skip\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent_comment\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:344\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:129\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    127\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cred_sims = create_cred_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add betweeness centrality, clustering coefficient, degree, and credibility as node attributes\n",
    "betweenness = {i: float(b) for i, b in nx.betweenness_centrality(graph).items()}\n",
    "nx.set_node_attributes(graph, betweenness, 'betweenness')\n",
    "\n",
    "clustering = {i: float(c) for i, c in nx.clustering(graph).items()}\n",
    "nx.set_node_attributes(graph, clustering, 'clustering')\n",
    "\n",
    "degree = dict(nx.degree(graph))\n",
    "nx.set_node_attributes(graph, degree, 'degree')\n",
    "\n",
    "credibility = {i: int(credibility > 0.5) for i, credibility in enumerate(credibilities)}\n",
    "nx.set_node_attributes(graph, credibility, 'credibility')\n",
    "\n",
    "nx.write_gexf(graph, 'data/reddit.gexf')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "find",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
