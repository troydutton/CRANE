{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6mynTN0keV4",
        "outputId": "b13e5b22-8fc4-4eba-fdf2-e5d4d4a9ab1b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56grQqYNkwCZ",
        "outputId": "af6a70ba-eb99-446d-dba7-c41481dedf99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "business: # submissions 265670\n",
            "business: # submissions after cleaning 265670\n",
            "Submissions processing done.\n",
            "Starting processing comments\n",
            "Chunk 0: 7913 comments after filtering\n",
            "Chunk 1: 103515 comments after filtering\n",
            "business: # comments 111428\n",
            "business: # comments after cleaning 111428\n",
            "Finished processing comments\n",
            "Overall Submissions: 265670\n",
            "Overall Comments: 111428\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_text(text_series):\n",
        "    \"\"\"\n",
        "    Cleans text data by removing URLs and HTML entities.\n",
        "\n",
        "    Parameters:\n",
        "    text_series (Series): Pandas Series containing text data.\n",
        "\n",
        "    Returns:\n",
        "    Series: Cleaned text data.\n",
        "    \"\"\"\n",
        "    # Remove URLs from text\n",
        "    text_series = text_series.str.replace(\n",
        "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|'\n",
        "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
        "    # Remove HTML entities like '&gt;'\n",
        "    text_series = text_series.str.replace('&gt;', '')\n",
        "    return text_series\n",
        "\n",
        "def add_datetime_columns(df, time_column='created_utc'):\n",
        "    \"\"\"\n",
        "    Adds 'Y' (year) and 'YM' (year-month) columns to the DataFrame based on a timestamp column.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): The DataFrame to modify.\n",
        "    time_column (str): Name of the timestamp column.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: The modified DataFrame with added 'Y' and 'YM' columns.\n",
        "    \"\"\"\n",
        "    # Convert timestamp to datetime and extract the year\n",
        "    df['Y'] = pd.to_datetime(df[time_column], unit='s').dt.year\n",
        "    # Extract year-month in 'YYYY-MM' format\n",
        "    df['YM'] = pd.to_datetime(df[time_column], unit='s').dt.strftime('%Y-%m')\n",
        "    return df\n",
        "\n",
        "def submission_comment_process(subreddit, min_num_comments=3, min_score=-1, years=[2016], chunk_size=10**6):\n",
        "    \"\"\"\n",
        "    Processes submissions and comments for a given subreddit and returns cleaned DataFrames.\n",
        "\n",
        "    Parameters:\n",
        "    subreddit (str): Name of the subreddit.\n",
        "    min_num_comments (int): Minimum number of comments required for a submission.\n",
        "    min_score (int): Minimum score required for a submission.\n",
        "    years (list): List of years to include in the data.\n",
        "    chunk_size (int): Number of rows per chunk when reading comments CSV.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame, DataFrame: Cleaned submissions and comments DataFrames.\n",
        "    \"\"\"\n",
        "    # Read submissions CSV file\n",
        "    df_s = pd.read_csv(f'data/{subreddit}_submissions.csv')\n",
        "    # Alternative path (commented out)\n",
        "    # df_s = pd.read_csv('drive/MyDrive/Reddit/path in g drive')\n",
        "\n",
        "    # Add 'Y' and 'YM' columns to submissions DataFrame\n",
        "    df_s = add_datetime_columns(df_s, 'created_utc')\n",
        "    # Filter submissions by specified years\n",
        "    df_s = df_s[df_s['Y'].isin(years)]\n",
        "    print(f\"{subreddit}: # submissions {len(df_s)}\")\n",
        "\n",
        "    # Filter submissions based on minimum score and number of comments\n",
        "    df_s = df_s[df_s['score'] > min_score]\n",
        "    df_s = df_s[df_s['num_comments'] >= min_num_comments]\n",
        "    # Add subreddit name to DataFrame\n",
        "    df_s['sub'] = subreddit\n",
        "    print(f\"{subreddit}: # submissions after cleaning {len(df_s)}\")\n",
        "    print('Submissions processing done.')\n",
        "\n",
        "    # Start processing comments\n",
        "    print('Starting processing comments')\n",
        "    # Read comments CSV file in chunks\n",
        "    comments_chunks = pd.read_csv(\n",
        "        f'data/{subreddit}_comments.csv', chunksize=chunk_size)\n",
        "    # Alternative path (commented out)\n",
        "    # comments_chunks = pd.read_csv('drive/MyDrive/Reddit/path in g drive', chunksize=chunk_size)\n",
        "\n",
        "    df_c = pd.DataFrame()  # Initialize empty DataFrame for comments\n",
        "\n",
        "    for i, chunk in enumerate(comments_chunks):\n",
        "        # Remove 't3_' prefix from 'link_id'\n",
        "        chunk['link_id'] = chunk['link_id'].str.replace('t3_', '')\n",
        "        # Keep only comments linked to the filtered submissions\n",
        "        chunk = chunk[chunk['link_id'].isin(df_s['id'].unique())]\n",
        "        print(f\"Chunk {i}: {len(chunk)} comments after filtering\")\n",
        "\n",
        "        # The following lines are commented out; they can be used for additional filtering\n",
        "        # Remove comments with missing 'body' (commented out)\n",
        "        # chunk = chunk[~chunk['body'].isna()]\n",
        "        # Remove comments where 'author' is '[deleted]' (commented out)\n",
        "        # chunk = chunk[chunk['author'] != '[deleted]']\n",
        "        # Remove comments where 'body' is '[removed]' (commented out)\n",
        "        # chunk = chunk[chunk['body'] != '[removed]']\n",
        "\n",
        "        # Remove prefix from 'parent_id'\n",
        "        chunk['parent_id'] = chunk['parent_id'].str[3:]\n",
        "        # Keep only comments with at least 3 words in 'body' (commented out)\n",
        "        # chunk = chunk[chunk['body'].str.split().str.len() >= 3]\n",
        "\n",
        "        # Add 'Y' and 'YM' columns to comments DataFrame\n",
        "        chunk = add_datetime_columns(chunk, 'created_utc')\n",
        "\n",
        "        # Break the loop if the chunk's years are beyond the specified range\n",
        "        if chunk['Y'].min() > max(years):\n",
        "            break\n",
        "\n",
        "        # Clean the 'body' text in comments\n",
        "        chunk['body'] = clean_text(chunk['body'])\n",
        "        # Add subreddit name to DataFrame\n",
        "        chunk['sub'] = subreddit\n",
        "\n",
        "        # Concatenate the processed chunk to the main comments DataFrame\n",
        "        df_c = pd.concat([df_c, chunk], ignore_index=True)\n",
        "\n",
        "    print(f\"{subreddit}: # comments {len(df_c)}\")\n",
        "\n",
        "    # Remove 't3_' prefix from 'link_id' in comments (redundant but kept for consistency)\n",
        "    df_c['link_id'] = df_c['link_id'].str.replace('t3_', '')\n",
        "    # Keep only comments linked to the filtered submissions\n",
        "    df_c = df_c[df_c['link_id'].isin(df_s['id'].unique())]\n",
        "\n",
        "    # The following lines are commented out; they can be used for additional filtering\n",
        "    # Remove comments where 'author' is '[deleted]' (commented out)\n",
        "    # df_c = df_c[df_c['author'] != '[deleted]']\n",
        "    # Remove comments where 'body' is '[deleted]' or '[removed]' (commented out)\n",
        "    # df_c = df_c[~df_c['body'].isin(['[deleted]', '[removed]'])]\n",
        "    # Remove comments with missing 'body' (commented out)\n",
        "    # df_c = df_c[~df_c['body'].isna()]\n",
        "    # Keep only comments with at least 3 words in 'body' (commented out)\n",
        "    # df_c = df_c[df_c['body'].str.split().str.len() >= 3]\n",
        "\n",
        "    # The following line is commented out; it might be for adding a 'Cred' column based on domain\n",
        "    # df_c['B'] = df_c['domain'].map(df_crd.set_index(\"Domain\")[\"Cred\"])\n",
        "\n",
        "    print(f\"{subreddit}: # comments after cleaning {len(df_c)}\")\n",
        "    print('Finished processing comments')\n",
        "    return df_s, df_c\n",
        "\n",
        "def P2P_Multi(Subreddits, min_num_comments=0, min_score=-1, years=[2017]):\n",
        "    \"\"\"\n",
        "    Processes multiple subreddits and aggregates the submissions and comments.\n",
        "\n",
        "    Parameters:\n",
        "    Subreddits (list): List of subreddit names.\n",
        "    min_num_comments (int): Minimum number of comments required for a submission.\n",
        "    min_score (int): Minimum score required for a submission.\n",
        "    years (list): List of years to include in the data.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame, DataFrame: Aggregated submissions and comments DataFrames.\n",
        "    \"\"\"\n",
        "    Df_S = pd.DataFrame()  # Initialize empty DataFrame for submissions\n",
        "    Df_C = pd.DataFrame()  # Initialize empty DataFrame for comments\n",
        "\n",
        "    for subreddit in Subreddits:\n",
        "        # Process each subreddit and get cleaned DataFrames\n",
        "        df_s, df_c = submission_comment_process(\n",
        "            subreddit, min_num_comments, min_score, years)\n",
        "        # Concatenate the results to the aggregated DataFrames\n",
        "        Df_S = pd.concat([Df_S, df_s], ignore_index=True)\n",
        "        Df_C = pd.concat([Df_C, df_c], ignore_index=True)\n",
        "\n",
        "    return Df_S, Df_C\n",
        "\n",
        "# List of subreddits to process\n",
        "Subreddits = [\"business\"]\n",
        "\n",
        "# Process the subreddits and get aggregated submissions and comments\n",
        "DS, DC = P2P_Multi(Subreddits, min_num_comments=0, min_score=-1, years=range(2016, 2017))\n",
        "\n",
        "# Filter submissions to only include those with comments in DC\n",
        "DS = DS[DS['sub'].isin(DC['sub'].unique())]\n",
        "\n",
        "print('Overall Submissions:', len(DS))\n",
        "print('Overall Comments:', len(DC))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index          111428\n",
              "id             111428\n",
              "author          20662\n",
              "created_utc    111152\n",
              "parent_id       69699\n",
              "link_id         34993\n",
              "body            86125\n",
              "Y                   4\n",
              "YM                 29\n",
              "sub                 1\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DC.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "-NMtdC7zxtlB",
        "outputId": "3d8bbe57-4dca-4365-f7a1-40789348ac02"
      },
      "outputs": [],
      "source": [
        "# sample submission data\n",
        "DS.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "wMcuCn4s8B3G",
        "outputId": "8759dceb-15f8-46a1-ebe2-2d2116647b79"
      },
      "outputs": [],
      "source": [
        "# sample comment data\n",
        "DC.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzq9hKKrIdgA"
      },
      "source": [
        "#Networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr4DSSAxskf6",
        "outputId": "93dd8ab4-6e9f-4abd-9b84-7e8f5fa46941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "f=pd.crosstab(DC['link_id'],DC['author'])\n",
        "A=np.array((f.values).astype(float))\n",
        "E=A@A.T\n",
        "np.fill_diagonal(E,0)\n",
        "print(E)\n",
        "min_num_shared_user=2\n",
        "Adj=np.array(E>min_num_shared_user)\n",
        "# G=nx.from_numpy_array(Adj)\n",
        "# print(# Graph Nodes:â€™, len(G.nodes))\n",
        "# for v in nx.nodes(G):\n",
        "    # G.nodes[v][\"sub\"] = DS_b.iloc[v][\"sub\"]\n",
        "# nx.write_gexf(G, 'business.gexf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "subset = Adj[:1000, :1000]\n",
        "G=nx.from_numpy_array(subset)\n",
        "nx.write_gexf(G, 'data/business.gexf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "G.number_of_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11499"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "G.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset.shape\n",
        "# G=nx.from_numpy_array(Adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "_fDIAVgt3Q44",
        "outputId": "5b4c8d5a-d5dc-47da-be19-a6ffcf29f575"
      },
      "outputs": [],
      "source": [
        "ds=DS[DS['id']==sbm]\n",
        "dc=DC[DC['link_id']==sbm]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgH5vKoenYwd",
        "outputId": "44bac1b7-475e-441e-e0d5-68e8a27df2f2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = DC[DC['link_id']==sbm]\n",
        "\n",
        "# Filter necessary columns\n",
        "comments = data[['id', 'parent_id', 'body']]\n",
        "\n",
        "# Initialize the comment tree with the root link_id\n",
        "link_id = sbm  # Assuming '5j5otc' is the link_id for root comments\n",
        "comment_tree = {link_id: []}\n",
        "\n",
        "# Function to add comment to the tree\n",
        "def add_to_tree(comment_id, parent_id, comment_body):\n",
        "    if parent_id == link_id:  # Handling root comments\n",
        "        if parent_id not in comment_tree:\n",
        "            comment_tree[parent_id] = []\n",
        "        comment_tree[parent_id].append({'id': comment_id, 'body': comment_body, 'replies': []})\n",
        "    else:\n",
        "        # Recursively find the parent and add the comment as a reply\n",
        "        find_and_add(parent_id, comment_tree[link_id], {'id': comment_id, 'body': comment_body, 'replies': []})\n",
        "\n",
        "# Recursive function to find the parent in the tree and add the current comment\n",
        "def find_and_add(parent_id, current_level, comment):\n",
        "    for entry in current_level:\n",
        "        if entry['id'] == parent_id:\n",
        "            entry['replies'].append(comment)\n",
        "            return True\n",
        "        if find_and_add(parent_id, entry['replies'], comment):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Build the tree by iterating over each comment\n",
        "for _, row in comments.iterrows():\n",
        "    add_to_tree(row['id'], row['parent_id'], row['body'])\n",
        "\n",
        "# Optional: Function to print the tree in a structured manner for better readability\n",
        "def print_tree(level, indent=0):\n",
        "    for node in level:\n",
        "        print(' ' * indent + f\"Comment ID: {node['id']}, Body: {node['body'][:60]}...\")  # Print truncated body\n",
        "        if node['replies']:\n",
        "            print_tree(node['replies'], indent + 4)\n",
        "\n",
        "# Example usage: Print the first few levels of the tree\n",
        "print_tree(comment_tree[link_id][:2])  # Adjust as needed for full tree or specific sections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "2PShqvXEsVra",
        "outputId": "50b5baec-2101-41a9-ffae-2b5f5df187f5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming DC and sbm are defined somewhere earlier in your code\n",
        "data = DC[DC['link_id'] == sbm]\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "# Filter necessary columns\n",
        "comments = data[['id', 'parent_id', 'body']]\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Define a root node for visualization and add nodes and edges\n",
        "root_node = 'Root'\n",
        "G.add_node(root_node, body=\"Root Post\")\n",
        "for _, row in comments.iterrows():\n",
        "    G.add_node(row['id'], body=row['id'])  # Use full ID for display\n",
        "    parent_id = row['parent_id'] if not row['parent_id'].startswith(sbm) else root_node\n",
        "    G.add_edge(parent_id, row['id'])\n",
        "\n",
        "# Compute depth for each node for shell assignment\n",
        "depth = nx.single_source_shortest_path_length(G, root_node)\n",
        "# Create shell layout based on depths\n",
        "max_depth = max(depth.values())\n",
        "shells = [[] for _ in range(max_depth + 1)]\n",
        "for node, d in depth.items():\n",
        "    shells[d].append(node)\n",
        "\n",
        "# Define node colors based on depth with a more vibrant color map\n",
        "node_color = [depth[node] / max_depth for node in G.nodes()]\n",
        "\n",
        "if 1:\n",
        "  # Draw the graph using the shell layout\n",
        "  pos = nx.shell_layout(G, shells)\n",
        "else:\n",
        "  pos = nx.spring_layout(G, k=0.1, iterations=50)  # Adjust k and iterations for better layout\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=100, cmap=plt.cm.viridis, alpha=0.8)\n",
        "nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
        "nx.draw_networkx_labels(G, pos, labels={n: G.nodes[n]['body'] for n in G.nodes}, font_size=8)\n",
        "\n",
        "plt.title(\"Enhanced Comment Tree Visualization\")\n",
        "plt.axis('off')  # Hide axes\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
